{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HDS-LEE Course on Hyperparameter Optimization - Part 1\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/DLR-SC/Hyperparameter_tutorial/master/img/hds_lee_title.png' width=500px>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "The aim of this tutorial is the following:\n",
    "\n",
    "* Presentation of a (typical) regression problem. In the following, we consider the 'Boston house price\n",
    "regression problem. \n",
    "* A (very) short introduction of Keras.\n",
    "* Hyperparameter optimization - The manual approach (i.e. without using a dedicated library like Talos)\n",
    "\n",
    "Note that most of the following code samples can be found in Chapter 3, Section 6 of \n",
    "[Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff).\n",
    "The author of this book has made his code available on [GitHub](https://github.com/fchollet/deep-learning-with-python-notebooks).\n",
    "This notebook is an adaption to allow for Hyperparameter optimization with Talos.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "##### 1. <a href=#one>Regression problem</a>\n",
    "##### 2. <a href=#two>Keras -  a very short introduction </a>\n",
    "##### 3. <a href=#three>Hyperparameter optimization (manual approach)</a>\n",
    "\n",
    "----"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction of the regression problem: Predicting Boston house prices <a name=\"one\"></a> \n",
    "\n",
    "We will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the \n",
    "suburb at the time, such as the crime rate, the local property tax rate, etc.\n",
    "\n",
    "The dataset we will be using has very few data points, only 506 in \n",
    "total, split between 404 training samples and 102 test samples, and each \"feature\" in the input data (e.g. the crime rate is a feature) has \n",
    "a different scale. For instance some values are proportions, which take a values between 0 and 1, others take values between 1 and 12, \n",
    "others between 0 and 100...\n",
    "\n",
    "Let's take a look at the data (index 0: number of samples, index 1: number of features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, we have 404 training samples and 102 test samples. The data comprises 13 features. The 13 features in the input data are as \n",
    "follow:\n",
    "\n",
    "1. Per capita crime rate.\n",
    "2. Proportion of residential land zoned for lots over 25,000 square feet.\n",
    "3. Proportion of non-retail business acres per town.\n",
    "4. Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "5. Nitric oxides concentration (parts per 10 million).\n",
    "6. Average number of rooms per dwelling.\n",
    "7. Proportion of owner-occupied units built prior to 1940.\n",
    "8. Weighted distances to five Boston employment centres.\n",
    "9. Index of accessibility to radial highways.\n",
    "10. Full-value property-tax rate per $10,000.\n",
    "11. Pupil-teacher ratio by town.\n",
    "12. 1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.\n",
    "13. % lower status of the population.\n",
    "\n",
    "The targets are the median values of owner-occupied homes, in thousands of dollars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The prices are typically between 10 000 dollar and 50 000 dollar. Compared to current prices this sounds very cheap but remender that we compare\n",
    "house price's from the 70s without including inflation."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Keras - a very short introduction <a name=\"two\"></a> "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "\n",
    "It would be problematic to use values for a neural network that all take wildly different ranges. The network might be able to\n",
    "automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal \n",
    "with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we \n",
    "will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a \n",
    "unit standard deviation. This is easily done in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that the quantities that we use for normalizing the test data have been computed using the training data. We should never use in our \n",
    "workflow any quantity computed on the test data, even for something as simple as data normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple neural network with Keras\n",
    "\n",
    "\n",
    "Because we have no previous knowledge about optimal parameters or an optimal network architecture, we will be using a very small network \n",
    "with just one hidden layers and 32 units. In general, the less training data you have, the worse overfitting will be, and using \n",
    "a small network is one way to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Because we will need to instantiate\n",
    "    # the same model multiple times,\n",
    "    # we use a function to construct it.\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(32, activation='relu',\n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our network ends with a single unit, and no activation function (i.e. it will be linear layer).\n",
    "This is a typical setup for scalar regression (i.e. regression where we are trying to predict a single continuous value). \n",
    "Applying an activation function would constrain the range that the output can take; for instance if \n",
    "we applied a `sigmoid` activation function to our last layer, the network could only learn to predict values between 0 and 1. Here, because \n",
    "the last layer is purely linear, the network is free to learn to predict values in any range.\n",
    "\n",
    "Note that we are compiling the network with the `mse` loss function -- Mean Squared Error, the square of the difference between the \n",
    "predictions and the targets, a widely used loss function for regression problems.\n",
    "\n",
    "We are also monitoring a metric during training: `mae`. This stands for Mean Absolute Error. It is simply the absolute value of the \n",
    "difference between the predictions and the targets. For instance, a MAE of 0.5 on this problem would mean that our predictions are off by \n",
    "\\$500 on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating our approach using K-fold validation\n",
    "\n",
    "\n",
    "To evaluate our network while we keep adjusting its parameters (such as the number of epochs used for training), we could simply split the \n",
    "data into a training set and a validation set. However, because we have so few data points, the \n",
    "validation set would end up being very small (e.g. about 100 examples). A consequence is that our validation scores may change a lot \n",
    "depending on _which_ data points we choose to use for validation and which we choose for training, i.e. the validation scores may have a \n",
    "high _variance_ with regard to the validation split. This would prevent us from reliably evaluating our model.\n",
    "\n",
    "The best practice in such situations is to use K-fold cross-validation. It consists of splitting the available data into K partitions \n",
    "(typically K=4 or 5), then instantiating K identical models, and training each one on K-1 partitions while evaluating on the remaining \n",
    "partition. The validation score for the model used would then be the average of the K validation scores obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of code, this is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: if the code runs too slow, you can just set k=2 \n",
    "k = 4 # typically K=4 or 5\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 50\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    # Evaluate the model on the validation data\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the outputs of the k different runs\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average as a more reliable values\n",
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can notice, the different runs do indeed show rather different validation scores, from 2.1 to 2.9. Their average ($$\\approx$$ 2.4) is a much more \n",
    "reliable metric than any single of these scores -- that's the entire point of K-fold cross-validation. In this case, we are off by 2,400 dollar on \n",
    "average, which is still significant considering that the prices range from 10,000 dollar to 50,000 dollar. \n",
    "\n",
    "Let's try to optimize some hyperparameters of our neural network to obtain a better results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Hyperparameter optimization (manual approach) <a name=\"three\"></a> "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "List of neural network hyperparameters that has to be opitimized (not complete):\n",
    "\n",
    "* Number of hidden layers\n",
    "* Number of neurons in each layer\n",
    "* batch size\n",
    "* number of epochs (length of training)\n",
    "* Regularization (dropout, L1/L2-regularization)\n",
    "* Optimizer + Learning rate\n",
    "* Loss function\n",
    "* Activation function\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tasks ## \n",
    "__Excercise 1:__\n",
    " - Play around with the different hyperparameters to build a better regression model\n",
    " - Can you achive a better result as <code>build_better_model() </code>\n",
    " - What are the important hyperparameters for this specific application?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def build_better_model_task():\n",
    "\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # replace 'number_of_neurons' with a value of your choice, e.g. 8, 16, 32, 64, 128\n",
    "    number_of_neurons = 32\n",
    "    model.add(layers.Dense(number_of_neurons, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    # maybe add a second hidden layer?\n",
    "    # model.add(layers.Dense(XXX, activation='relu'))    \n",
    "    \n",
    "    # or a third hidden layer?\n",
    "    # model.add(layers.Dense(XXX, activation='relu'))      \n",
    "    \n",
    "    # try dropout regularization, here: XXX \\in [0, 1.]\n",
    "    # model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    # or a layer with L1/L2-regularization\n",
    "    # model.add(layers.Dense(32, activation='relu', kernel_regularizer = regularizers.l2(0.001)))   \n",
    "    \n",
    "    # or a different activation function elu, tanh, sigmoid, ...\n",
    "    # model.add(layers.Dense(XXX, activation='elu'))  \n",
    "    \n",
    "    model.add(layers.Dense(1))    \n",
    "    \n",
    "    # try other optimizers such das Adam, Nadam, ... or maybe or different loss function?\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look how your better model performs. Keep in mind that the number of epochs (num_epochs)\n",
    "is also a hyperparameter."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a copy of the previous cell in which 'build_model' has been replaced with 'build_better_model_task()'\n",
    "\n",
    "num_epochs = 50 # <- further hyperparameter, try different values\n",
    "\n",
    "k = 4 # typically K=4 or 5\n",
    "num_val_samples = len(train_data) // k\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the improved Keras model (already compiled)\n",
    "    model = build_better_model_task()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    # Evaluate the model on the validation data\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the outputs of the k different runs\n",
    "all_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the average as a more reliable values\n",
    "np.mean(all_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try optimized model on the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done tuning other parameters of our model, we \n",
    "can train a final \"production\" model on all of the training data, with the best parameters, then look at its performance on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh, compiled model.\n",
    "model = build_better_model_task()\n",
    "# Train it on the entirety of the data.\n",
    "model.fit(train_data, train_targets,\n",
    "          epochs=50, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result on test set\n",
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "* When there is little data available, using K-Fold validation is a great way to reliably evaluate a model.\n",
    "* When little training data is available, it is preferable to use a small network with very few hidden layers (typically only one or two), \n",
    "in order to avoid severe overfitting.\n",
    "* Finding the optimal hyperparameters manually can be very difficult\n",
    "\n",
    "In the next chapter, you will use Talos to simplify the finding of adequate hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}